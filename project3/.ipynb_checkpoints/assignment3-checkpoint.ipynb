{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18ed92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d59a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_insulin_data():\n",
    "    insulin_df = pd.read_csv(\"InsulinData.csv\", parse_dates=[['Date', 'Time']], \\\n",
    "                             keep_date_col=True, low_memory=False)\n",
    "    insulin_df = insulin_df[['Date_Time', 'BWZ Carb Input (grams)']]\n",
    "    insulin_df = insulin_df.rename(columns={'BWZ Carb Input (grams)': 'Carb_Input'})\n",
    "    print('extracting insulin df: ' + str(insulin_df.shape))\n",
    "    return insulin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82b009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cgm_data(): \n",
    "    cgm_df = pd.read_csv('CGMData.csv', parse_dates=[['Date', 'Time']], keep_date_col=True, low_memory=False)\n",
    "    cgm_df = cgm_df[['Date_Time', 'Index','Sensor Glucose (mg/dL)', 'Date', 'Time']]\n",
    "    cgm_df = cgm_df.rename(columns={'Sensor Glucose (mg/dL)': 'Sensor_Glucose'})\n",
    "    print('extracting cgm df: ' + str(cgm_df.size))\n",
    "    return cgm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f506e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting insulin df: (41435, 2)\n",
      "extracting cgm df: 276715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82870"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "276715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "insulin_df, cgm_df = load_insulin_data(), load_cgm_data()\n",
    "display(insulin_df.size)\n",
    "display(cgm_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4426cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d05e966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = dt.datetime(2100, 12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c9857015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meal_start_times(insulin_df, cgm_df):\n",
    "    \n",
    "    insulin_df = insulin_df[(insulin_df['Carb_Input'].notna()) & (insulin_df['Carb_Input'] != 0)]\n",
    "    insulin_df = insulin_df.set_index('Date_Time').sort_index().reset_index()\n",
    "    \n",
    "    # include only those meal periods for which the next carb intake is atleast after 2 hrs\n",
    "    mask = (insulin_df['Date_Time'].shift(-1, fill_value=inf) - insulin_df['Date_Time'] \\\n",
    "            >= dt.timedelta(hours=2))\n",
    "    \n",
    "    insulin_df = insulin_df[mask]\n",
    "    \n",
    "    # column rename is required for the following merge\n",
    "    insulin_df = insulin_df.rename(columns = {'Date_Time': 'Pseudo_Start_Time'})\n",
    "    \n",
    "    cgm_df = cgm_df[cgm_df['Sensor_Glucose'].notna()]\n",
    "    cgm_df = cgm_df.set_index('Date_Time').sort_index().reset_index()\n",
    "    \n",
    "    meal_df = pd.merge_asof(insulin_df, cgm_df, left_on='Pseudo_Start_Time', \\\n",
    "                            right_on='Date_Time', direction='forward')[['Date_Time', 'Carb_Input']]\n",
    "    \n",
    "    min, max = meal_df['Carb_Input'].min(), meal_df['Carb_Input'].max()\n",
    "    \n",
    "    # binning BWZ Carb Input (grams) into bins of range 20\n",
    "    meal_df['Carb_Input'] = (meal_df['Carb_Input'] - min) // 20\n",
    "    \n",
    "    return meal_df\n",
    "    \n",
    "meal_df = extract_meal_start_times(insulin_df, cgm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meal_data_matrix(cgm_df, meal_df):\n",
    "    \n",
    "    meal_data_list = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    for _, row in meal_df.iterrows():\n",
    "        \n",
    "        meal_start = row['Date_Time'] - pd.DateOffset(minutes=30)\n",
    "        meal_end = row['Date_Time'] + pd.DateOffset(hours=2)\n",
    "        meal = cgm_df.loc[(cgm_df['Date_Time'] >= meal_start) & (cgm_df['Date_Time'] < meal_end)]\n",
    "        \n",
    "        # remove meal periods with <30 readings\n",
    "        if (meal_df.shape[0] < 30):\n",
    "            continue\n",
    "            \n",
    "        meal = meal[meal['Sensor_Glucose'].notna()]\n",
    "        meal = meal.set_index('Date_Time').sort_index().reset_index()\n",
    "\n",
    "        # remove readings <300 seconds apart\n",
    "        mask = (meal['Date_Time'].shift(-1, fill_value=inf) - meal['Date_Time'] \\\n",
    "                >= dt.timedelta(seconds=300))\n",
    "        \n",
    "        meal = meal[mask]\n",
    "\n",
    "        # only include meal_period if it has exactly 30 readings\n",
    "        if (meal.shape[0] == 30):\n",
    "            meal_data_list.append(meal['Sensor_Glucose'])\n",
    "            ground_truth.append(row['Carb_Input'])\n",
    "        \n",
    "    feature_matrix = pd.concat(meal_data_list, axis=1).transpose()\n",
    "    feature_matrix['label'] = ground_truth \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6da7cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_data_matrix = compute_meal_data_matrix(cgm_df, meal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "767551f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, iqr\n",
    "from scipy.signal import periodogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "6ef24c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meal_feature_matrix(meal_data_matrix):\n",
    "    \n",
    "    # exclude ground truth\n",
    "    _input = meal_data_matrix.iloc[:, :-1]\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    velocity = _input.diff(axis=1).dropna(axis=1, how='all')\n",
    "    features['velocity_min'] = velocity.min(axis=1)\n",
    "    features['velocity_max'] = velocity.max(axis=1)\n",
    "    features['velocity_mean'] = velocity.mean(axis=1)\n",
    "\n",
    "    acceleration = velocity.diff(axis=1).dropna(axis=1, how='all')\n",
    "    features['acceleration_min'] = acceleration.min(axis=1)\n",
    "    features['acceleration_max'] = acceleration.max(axis=1)\n",
    "    features['acceleration_mean'] = acceleration.mean(axis=1)\n",
    "\n",
    "    features['entropy'] = _input.apply(lambda row: entropy(row, base=2), axis=1)\n",
    "    features['iqr'] = _input.apply(lambda row: entropy(row, base=2), axis=1)\n",
    "    \n",
    "    fft_values = _input.apply(lambda row: np.fft.fft(row), axis=1)\n",
    "\n",
    "    # get the indices of the frequencies sorted by decreasing amplitude\n",
    "    fft_indices = fft_values.apply(lambda row: np.argsort(np.abs(row))[::-1])\n",
    "\n",
    "    # select the first 6 peaks of each row\n",
    "    fft_peaks = fft_indices.apply(lambda row: row[:6])\n",
    "    fft_peaks = fft_peaks.apply(pd.Series)\n",
    "    fft_peaks.columns = ['fft_max_' + str(i+1) for i in fft_peaks.apply(pd.Series).columns]\n",
    "    \n",
    "    features = pd.concat([features, fft_peaks], axis=1)\n",
    "    \n",
    "    _input = meal_data_matrix.iloc[:, :-1]\n",
    "    psd = _input.apply(lambda row: periodogram(row)[1], axis=1)\n",
    "    psd = psd.apply(lambda row: [np.mean(row[0:5]), np.mean(row[5:10]), np.mean(row[10:16])])\n",
    "    psd = psd.apply(pd.Series)\n",
    "    psd.columns = ['psd1', 'psd2', 'psd3']\n",
    " \n",
    "    features = pd.concat([features, psd], axis=1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "3be5e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_feature_matrix = compute_meal_feature_matrix(meal_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "3c1d6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "656868e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "meal_feature_matrix_scaled = scaler.fit_transform(meal_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91811da8",
   "metadata": {},
   "source": [
    "\n",
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "41403ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7, n_init=20, max_iter=100)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(meal_feature_matrix_scaled)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "1bb0f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE:  3272.903967382067\n"
     ]
    }
   ],
   "source": [
    "sse = kmeans.inertia_\n",
    "print(\"SSE: \", sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "fb6075cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "8e037a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = meal_data_matrix['label']\n",
    "cont_matrix = contingency_matrix(ground_truth, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "b220c24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.3389423076923077\n",
      "Entropy:  2.169861476829342\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(meal_data_matrix)\n",
    "\n",
    "# Calculate the purity\n",
    "purity = np.sum(np.amax(cont_matrix, axis=0)) / num_samples\n",
    "\n",
    "# Calculate the entropy\n",
    "entropy = -np.sum((np.sum(cont_matrix, axis=1) / num_samples) *\n",
    "                  np.log2(np.sum(cont_matrix, axis=1) / num_samples))\n",
    "\n",
    "print(\"Purity: \", purity)\n",
    "print(\"Entropy: \", entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN clustering on the selected meal input\n",
    "db_scan = DBSCAN(eps=0.03, min_samples=8).fit(select_meal_input)\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = db_scan.labels_\n",
    "\n",
    "# Calculate the number of clusters\n",
    "number_of_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "# Calculate the number of noise points\n",
    "number_of_noise_points = list(labels).count(-1)\n",
    "\n",
    "distance = []\n",
    "ssedbscanP1 = 0\n",
    "\n",
    "# Calculate the sum of squared errors for DBSCAN\n",
    "for label in np.unique(db_scan.labels_):\n",
    "    if label == -1:  # Ignore noise points\n",
    "        continue\n",
    "    # Find the indices of points with the current label\n",
    "    center_points = np.where(db_scan.labels_ == label)\n",
    "    # Calculate the cluster center\n",
    "    center = np.mean(select_meal_input[center_points], axis=0)\n",
    "    ssedbscanP1 += np.sum(np.square(euclidean_distances(\n",
    "        [center], select_meal_input[center_points])), axis=1)\n",
    "\n",
    "temp_index = normalize_data + 1  # Add 1 to the normalized data\n",
    "# Calculate the entropy for DBSCAN clustering\n",
    "entropy_db_scan_P1 = calculateEntropy(db_scan.labels_, temp_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e9ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e8b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 (cse546-data-mining)",
   "language": "python",
   "name": "cse546-data-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

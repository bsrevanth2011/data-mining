{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18ed92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d59a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_insulin_data():\n",
    "    insulin_df = pd.read_csv(\"InsulinData.csv\", parse_dates=[['Date', 'Time']], \\\n",
    "                             keep_date_col=True, low_memory=False)\n",
    "    insulin_df = insulin_df[['Date_Time', 'BWZ Carb Input (grams)']]\n",
    "    insulin_df = insulin_df.rename(columns={'BWZ Carb Input (grams)': 'Carb_Input'})\n",
    "    print('extracting insulin df: ' + str(insulin_df.shape))\n",
    "    return insulin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c858e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cgm_data(): \n",
    "    cgm_df = pd.read_csv('CGMData.csv', parse_dates=[['Date', 'Time']], keep_date_col=True, low_memory=False)\n",
    "    cgm_df = cgm_df[['Date_Time', 'Index','Sensor Glucose (mg/dL)', 'Date', 'Time']]\n",
    "    cgm_df = cgm_df.rename(columns={'Sensor Glucose (mg/dL)': 'Sensor_Glucose'})\n",
    "    print('extracting cgm df: ' + str(cgm_df.size))\n",
    "    return cgm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f506e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting insulin df: (41435, 2)\n",
      "extracting cgm df: 276715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82870"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "276715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "insulin_df, cgm_df = load_insulin_data(), load_cgm_data()\n",
    "display(insulin_df.size)\n",
    "display(cgm_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50762fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ac041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = dt.datetime(2100, 12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e90202fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meal_start_times(insulin_df, cgm_df):\n",
    "    \n",
    "    insulin_df = insulin_df[(insulin_df['Carb_Input'].notna()) & (insulin_df['Carb_Input'] != 0)]\n",
    "    insulin_df = insulin_df.set_index('Date_Time').sort_index().reset_index()\n",
    "    \n",
    "    # include only those meal periods for which the next carb intake is atleast after 2 hrs\n",
    "    mask = (insulin_df['Date_Time'].shift(-1, fill_value=inf) - insulin_df['Date_Time'] \\\n",
    "            >= dt.timedelta(hours=2))\n",
    "    \n",
    "    insulin_df = insulin_df[mask]\n",
    "    \n",
    "    # column rename is required for the following merge\n",
    "    insulin_df = insulin_df.rename(columns = {'Date_Time': 'Pseudo_Start_Time'})\n",
    "    \n",
    "    cgm_df = cgm_df[cgm_df['Sensor_Glucose'].notna()]\n",
    "    cgm_df = cgm_df.set_index('Date_Time').sort_index().reset_index()\n",
    "    \n",
    "    meal_df = pd.merge_asof(insulin_df, cgm_df, left_on='Pseudo_Start_Time', \\\n",
    "                            right_on='Date_Time', direction='forward')[['Date_Time', 'Carb_Input']]\n",
    "    \n",
    "    min, max = meal_df['Carb_Input'].min(), meal_df['Carb_Input'].max()\n",
    "    \n",
    "    # binning BWZ Carb Input (grams) into bins of range 20\n",
    "    meal_df['Carb_Input'] = (meal_df['Carb_Input'] - min) // 20\n",
    "    \n",
    "    return meal_df\n",
    "    \n",
    "meal_df = extract_meal_start_times(insulin_df, cgm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "4d9ab1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meal_data_matrix(cgm_df, meal_df):\n",
    "    \n",
    "    meal_data_list = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    for _, row in meal_df.iterrows():\n",
    "        \n",
    "        meal_start = row['Date_Time'] - pd.DateOffset(minutes=30)\n",
    "        meal_end = row['Date_Time'] + pd.DateOffset(hours=2)\n",
    "        meal = cgm_df.loc[(cgm_df['Date_Time'] >= meal_start) & (cgm_df['Date_Time'] < meal_end)]\n",
    "        \n",
    "        # remove meal periods with <30 readings\n",
    "        if (meal_df.shape[0] < 30):\n",
    "            continue\n",
    "            \n",
    "        meal = meal[meal['Sensor_Glucose'].notna()]\n",
    "        meal = meal.set_index('Date_Time').sort_index().reset_index()\n",
    "\n",
    "        # remove readings <300 seconds apart\n",
    "        mask = (meal['Date_Time'].shift(-1, fill_value=inf) - meal['Date_Time'] \\\n",
    "                >= dt.timedelta(seconds=300))\n",
    "        \n",
    "        meal = meal[mask]\n",
    "\n",
    "        # only include meal_period if it has exactly 30 readings\n",
    "        if (meal.shape[0] == 30):\n",
    "            meal_data_list.append(meal['Sensor_Glucose'])\n",
    "            ground_truth.append(row['Carb_Input'])\n",
    "        \n",
    "    feature_matrix = pd.concat(meal_data_list, axis=1).transpose()\n",
    "    feature_matrix['label'] = ground_truth \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "1d984b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_data_matrix = compute_meal_data_matrix(cgm_df, meal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9a5b7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, iqr\n",
    "from scipy.signal import periodogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "48c9e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meal_feature_matrix(meal_data_matrix):\n",
    "    \n",
    "    # exclude ground truth\n",
    "    _input = meal_data_matrix.iloc[:, :-1]\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    velocity = _input.diff(axis=1).dropna(axis=1, how='all')\n",
    "    features['velocity_min'] = velocity.min(axis=1)\n",
    "    features['velocity_max'] = velocity.max(axis=1)\n",
    "    features['velocity_mean'] = velocity.mean(axis=1)\n",
    "\n",
    "    acceleration = velocity.diff(axis=1).dropna(axis=1, how='all')\n",
    "    features['acceleration_min'] = acceleration.min(axis=1)\n",
    "    features['acceleration_max'] = acceleration.max(axis=1)\n",
    "    features['acceleration_mean'] = acceleration.mean(axis=1)\n",
    "\n",
    "    features['entropy'] = _input.apply(lambda row: entropy(row, base=2), axis=1)\n",
    "    features['iqr'] = _input.apply(lambda row: entropy(row, base=2), axis=1)\n",
    "    \n",
    "    fft_values = _input.apply(lambda row: np.fft.fft(row), axis=1)\n",
    "\n",
    "    # get the indices of the frequencies sorted by decreasing amplitude\n",
    "    fft_indices = fft_values.apply(lambda row: np.argsort(np.abs(row))[::-1])\n",
    "\n",
    "    # select the first 6 peaks of each row\n",
    "    fft_peaks = fft_indices.apply(lambda row: row[:6])\n",
    "    fft_peaks = fft_peaks.apply(pd.Series)\n",
    "    fft_peaks.columns = ['fft_max_' + str(i+1) for i in fft_peaks.apply(pd.Series).columns]\n",
    "    \n",
    "    features = pd.concat([features, fft_peaks], axis=1)\n",
    "    \n",
    "    _input = meal_data_matrix.iloc[:, :-1]\n",
    "    psd = _input.apply(lambda row: periodogram(row)[1], axis=1)\n",
    "    psd = psd.apply(lambda row: [np.mean(row[0:5]), np.mean(row[5:10]), np.mean(row[10:16])])\n",
    "    psd = psd.apply(pd.Series)\n",
    "    psd.columns = ['psd1', 'psd2', 'psd3']\n",
    " \n",
    "    features = pd.concat([features, psd], axis=1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f702d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_feature_matrix = compute_meal_feature_matrix(meal_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "eaf1c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c1390c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "meal_feature_matrix_scaled = scaler.fit_transform(meal_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a0c04",
   "metadata": {},
   "source": [
    "\n",
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "6528fd1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7, n_init=20, max_iter=100)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(meal_feature_matrix_scaled)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "ad66c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "3fc7d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = meal_data_matrix['label']\n",
    "cont_matrix = contingency_matrix(ground_truth, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(meal_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "dd3aba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans SSE:  3272.903967382067\n",
      "KMeans Purity:  0.2644230769230769\n",
      "KMeans Entropy:  1.9991059939180764\n"
     ]
    }
   ],
   "source": [
    "# Calculate the SSE\n",
    "kmeans_sse = kmeans.inertia_\n",
    "\n",
    "# Calculate the purity\n",
    "kmeans_purity = np.sum(np.amax(cont_matrix, axis=0)) / num_samples\n",
    "\n",
    "# Calculate the entropy\n",
    "kmeans_entropy = -np.sum((np.sum(cont_matrix, axis=1) / num_samples) *\n",
    "                  np.log2(np.sum(cont_matrix, axis=1) / num_samples))\n",
    "\n",
    "print(\"KMeans SSE: \", kmeans_sse)\n",
    "print(\"KMeans Purity: \", kmeans_purity)\n",
    "print(\"KMeans Entropy: \", kmeans_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050e353",
   "metadata": {},
   "source": [
    "\n",
    "## DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "fa925ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "314dcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=2.4, min_samples=5).fit(meal_feature_matrix_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "3486e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dbscan.labels_.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "b1ed6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[labels == -1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "3322a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_matrix = pd.crosstab(ground_truth, labels, dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "10244e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN SSE:  0.6356096671073097\n",
      "DBSCAN Purity:  0.2644230769230769\n",
      "DBSCAN Entropy:  1.9991059939180764\n"
     ]
    }
   ],
   "source": [
    "# Calculate the SSE\n",
    "centroids = meal_feature_matrix_scaled.copy()\n",
    "\n",
    "# Change np.nan back to -1\n",
    "labels = np.nan_to_num(labels, nan=-1)\n",
    "\n",
    "for label in np.unique(labels[1:]):\n",
    "    centroids[labels == label] =  centroids[labels == label].mean(axis=0)\n",
    "    \n",
    "dbscan_sse = mean_squared_error(meal_feature_matrix_scaled, centroids)\n",
    "\n",
    "# Calculate the purity\n",
    "dbscan_purity = np.sum(np.amax(cont_matrix, axis=0)) / num_samples\n",
    "\n",
    "# Calculate the entropy\n",
    "dbscan_entropy = -np.sum((np.sum(cont_matrix, axis=1) / num_samples) *\n",
    "                  np.log2(np.sum(cont_matrix, axis=1) / num_samples))\n",
    "\n",
    "print(\"DBSCAN SSE: \", dbscan_sse)\n",
    "print(\"DBSCAN Purity: \", dbscan_purity)\n",
    "print(\"DBSCAN Entropy: \", dbscan_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "5e2a76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clustering evaluation results in a CSV file\n",
    "results = np.array([[kmeans_sse, dbscan_sse, kmeans_entropy, dbscan_entropy, kmeans_purity, dbscan_purity]])\n",
    "np.savetxt(\"Results.csv\", results, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 (cse546-data-mining)",
   "language": "python",
   "name": "cse546-data-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
